"""
ë¬¸ì„œ ë¶„ì„ API ë¼ìš°í„° - íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„
"""
import logging
import datetime
import json
from fastapi import APIRouter, HTTPException, status, UploadFile, File
from pydantic import BaseModel
from ..services.document_analyzer import (
    upload_resume_file, 
    upload_job_posting_file, 
    analyze_candidate_match,
    document_analyzer,
    get_storage_files_list
)

logger = logging.getLogger(__name__)

# ë¼ìš°í„° ìƒì„±
router = APIRouter(
    prefix="/document",
    tags=["ë¬¸ì„œë¶„ì„"],
    responses={
        404: {"description": "Not found"},
        500: {"description": "Internal server error"}
    }
)

@router.post("/upload-resume")
async def upload_resume_api(file: UploadFile = File(...)):
    """
    ì´ë ¥ì„œ íŒŒì¼ ì—…ë¡œë“œ (1ë‹¨ê³„)
    
    Args:
        file: ì´ë ¥ì„œ íŒŒì¼ (PDF, Word ë“±)
        
    Returns:
        dict: ì—…ë¡œë“œ ê²°ê³¼ (íŒŒì¼ëª… í¬í•¨)
    """
    try:
        filename = file.filename or "unknown_resume.pdf"
        logger.info(f"ì´ë ¥ì„œ ì—…ë¡œë“œ ìš”ì²­: {filename}")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_content = await file.read()
        
        # Azure Blob Storageì— ì—…ë¡œë“œ
        result = upload_resume_file(file_content, filename)
        
        logger.info(f"ì´ë ¥ì„œ ì—…ë¡œë“œ ì™„ë£Œ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"ì´ë ¥ì„œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì´ë ¥ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        )

@router.post("/upload-job")
async def upload_job_posting_api(file: UploadFile = File(...)):
    """
    ì±„ìš©ê³µê³  íŒŒì¼ ì—…ë¡œë“œ (1ë‹¨ê³„)
    
    Args:
        file: ì±„ìš©ê³µê³  íŒŒì¼ (PDF, Word ë“±)
        
    Returns:
        dict: ì—…ë¡œë“œ ê²°ê³¼ (íŒŒì¼ëª… í¬í•¨)
    """
    try:
        filename = file.filename or "unknown_job.pdf"
        logger.info(f"ì±„ìš©ê³µê³  ì—…ë¡œë“œ ìš”ì²­: {filename}")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_content = await file.read()
        
        # Azure Blob Storageì— ì—…ë¡œë“œ
        result = upload_job_posting_file(file_content, filename)
        
        logger.info(f"ì±„ìš©ê³µê³  ì—…ë¡œë“œ ì™„ë£Œ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"ì±„ìš©ê³µê³  ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì±„ìš©ê³µê³  ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        )

class AnalyzeFilesRequest(BaseModel):
    resume_filename: str
    job_filename: str

@router.post("/analyze-files")
async def analyze_files_api(request: AnalyzeFilesRequest):
    """
    ì—…ë¡œë“œëœ íŒŒì¼ë“¤ë¡œ ë¶„ì„ ì‹¤í–‰ (2ë‹¨ê³„)
    
    Args:
        request: ë¶„ì„ ìš”ì²­ ë°ì´í„° (ì´ë ¥ì„œ íŒŒì¼ëª…, ì±„ìš©ê³µê³  íŒŒì¼ëª…)
        
    Returns:
        dict: ë¶„ì„ ê²°ê³¼ (ì í•©ë„, ê°•ì , ìš°ë ¤ì‚¬í•­, ì¶”ì²œì§ˆë¬¸ í¬í•¨)
    """
    try:
        logger.info(f"íŒŒì¼ ë¶„ì„ ìš”ì²­: {request.resume_filename} vs {request.job_filename}")
        
        # íŒŒì¼ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰
        result = analyze_candidate_match(request.resume_filename, request.job_filename)
        
        logger.info("íŒŒì¼ ë¶„ì„ ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        )

class AnalyzeTextRequest(BaseModel):
    resume_text: str
    job_posting_text: str

@router.post("/analyze-text")
async def analyze_text_api(request: AnalyzeTextRequest):
    """
    ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ë¶„ì„ (íŒŒì¼ ì—†ì´)
    
    Args:
        request: ë¶„ì„ ìš”ì²­ ë°ì´í„° (ì´ë ¥ì„œ ë‚´ìš©, ì±„ìš©ê³µê³  ë‚´ìš©)
        
    Returns:
        dict: ë¶„ì„ ê²°ê³¼ (ì í•©ë„, ê°•ì , ìš°ë ¤ì‚¬í•­, ì¶”ì²œì§ˆë¬¸ í¬í•¨)
    """
    try:
        logger.info("ì§ì ‘ í…ìŠ¤íŠ¸ ë¶„ì„ ìš”ì²­")
        
        # ì§ì ‘ í…ìŠ¤íŠ¸ ë¶„ì„
        result = document_analyzer.analyze_match(request.resume_text, request.job_posting_text)
        
        logger.info("ì§ì ‘ í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        )

@router.post("/upload-both")
async def upload_both_files_api(
    resume_file: UploadFile = File(...),
    job_file: UploadFile = File(...)
):
    """
    ì´ë ¥ì„œ + ì±„ìš©ê³µê³  ë™ì‹œ ì—…ë¡œë“œ (í¸ì˜ ê¸°ëŠ¥)
    
    Args:
        resume_file: ì´ë ¥ì„œ íŒŒì¼
        job_file: ì±„ìš©ê³µê³  íŒŒì¼
        
    Returns:
        dict: ë‘ íŒŒì¼ ì—…ë¡œë“œ ê²°ê³¼
    """
    try:
        resume_filename = resume_file.filename or "unknown_resume.pdf"
        job_filename = job_file.filename or "unknown_job.pdf"
        logger.info(f"ë™ì‹œ ì—…ë¡œë“œ ìš”ì²­: {resume_filename}, {job_filename}")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        resume_content = await resume_file.read()
        job_content = await job_file.read()
        
        # ê°ê° ì—…ë¡œë“œ
        resume_result = upload_resume_file(resume_content, resume_filename)
        job_result = upload_job_posting_file(job_content, job_filename)
        
        result = {
            "resume_upload": resume_result,
            "job_upload": job_result,
            "status": "success" if resume_result["status"] == "success" and job_result["status"] == "success" else "error"
        }
        
        logger.info("ë™ì‹œ ì—…ë¡œë“œ ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"ë™ì‹œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë™ì‹œ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        )

@router.post("/upload-and-analyze")
async def upload_and_analyze_api(
    resume_file: UploadFile = File(...),
    job_file: UploadFile = File(...)
):
    """
    íŒŒì¼ ì—…ë¡œë“œ + ë¶„ì„ í•œ ë²ˆì— ì²˜ë¦¬ (ì˜¬ì¸ì› ê¸°ëŠ¥) - ì‹œì—°ìš© ìµœì í™”
    
    Args:
        resume_file: ì´ë ¥ì„œ íŒŒì¼
        job_file: ì±„ìš©ê³µê³  íŒŒì¼
        
    Returns:
        dict: ì—…ë¡œë“œ ê²°ê³¼ + ë¶„ì„ ê²°ê³¼
    """
    try:
        resume_filename = resume_file.filename or "unknown_resume.pdf"
        job_filename = job_file.filename or "unknown_job.pdf"
        logger.info(f"ğŸš€ ì—…ë¡œë“œ+ë¶„ì„ ìš”ì²­: {resume_filename}, {job_filename}")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        resume_content = await resume_file.read()
        job_content = await job_file.read()
        
        # 1ë‹¨ê³„: ì—…ë¡œë“œ
        logger.info("ğŸ“¤ 1ë‹¨ê³„: íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
        resume_upload = upload_resume_file(resume_content, resume_filename)
        job_upload = upload_job_posting_file(job_content, job_filename)
        
        if resume_upload["status"] != "success" or job_upload["status"] != "success":
            return {
                "status": "error",
                "message": "íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨",
                "resume_upload": resume_upload,
                "job_upload": job_upload
            }
        
        # 2ë‹¨ê³„: ì¸ë±ì„œ ì¦‰ì‹œ ì‹¤í–‰ (ì‹œì—°ìš© ìµœì í™”)
        logger.info("âš¡ 2ë‹¨ê³„: ì¸ë±ì„œ ì¦‰ì‹œ ì‹¤í–‰ ì¤‘...")
        indexer_result = document_analyzer.run_indexer()
        logger.info(f"   ì¸ë±ì„œ ì‹¤í–‰ ê²°ê³¼: {indexer_result.get('status', 'unknown')}")
        
        # 3ë‹¨ê³„: ì¸ë±ìŠ¤ ì¬ë°œê²¬ (ìƒˆë¡œìš´ ì¸ë±ìŠ¤ê°€ ìƒì„±ë  ìˆ˜ ìˆìŒ)
        logger.info("ğŸ” 3ë‹¨ê³„: ìµœì‹  ì¸ë±ìŠ¤ ì¬ë°œê²¬ ì¤‘...")
        old_index = document_analyzer.index_name
        document_analyzer.index_name = document_analyzer._get_active_index_name()
        
        # ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ë„ ìƒˆë¡œìš´ ì¸ë±ìŠ¤ë¡œ ì—…ë°ì´íŠ¸
        from azure.search.documents import SearchClient
        document_analyzer.search_client = SearchClient(
            endpoint=document_analyzer.search_endpoint,
            index_name=document_analyzer.index_name,
            credential=document_analyzer.search_credential
        )
        
        if old_index != document_analyzer.index_name:
            logger.info(f"   ì¸ë±ìŠ¤ ë³€ê²½: {old_index} â†’ {document_analyzer.index_name}")
        else:
            logger.info(f"   ê¸°ì¡´ ì¸ë±ìŠ¤ ìœ ì§€: {document_analyzer.index_name}")
        
        # 4ë‹¨ê³„: ì¸ë±ì‹± ì™„ë£Œ ëŒ€ê¸°
        logger.info("â³ 4ë‹¨ê³„: ì¸ë±ì‹± ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
        from ..services.document_analyzer import wait_for_file_indexing
        import time
        
        # ê° íŒŒì¼ì˜ ì¸ë±ì‹± ì™„ë£Œ ëŒ€ê¸° (ì‹œì—°ìš©: ìµœëŒ€ 30ì´ˆ)
        resume_indexed = wait_for_file_indexing(f"resume_{resume_filename}", 30)
        job_indexed = wait_for_file_indexing(f"job_{job_filename}", 30)
        
        logger.info(f"   ì¸ë±ì‹± ìƒíƒœ - ì´ë ¥ì„œ: {resume_indexed}, ì±„ìš©ê³µê³ : {job_indexed}")
        
        # 5ë‹¨ê³„: ë¶„ì„ ì‹¤í–‰ (ì¸ë±ì‹± ìƒíƒœì™€ ê´€ê³„ì—†ì´ ì‹œë„)
        logger.info("ğŸ“Š 5ë‹¨ê³„: ë¶„ì„ ì‹¤í–‰ ì¤‘...")
        if not resume_indexed or not job_indexed:
            logger.warning("âš ï¸ ì¸ë±ì‹± ë¯¸ì™„ë£Œ ìƒíƒœì—ì„œ ë¶„ì„ ì‹œë„...")
            
        analysis_result = analyze_candidate_match(resume_filename, job_filename)
        
        result = {
            "status": "success",
            "upload_results": {
                "resume_upload": resume_upload,
                "job_upload": job_upload
            },
            "indexer_result": indexer_result,
            "index_info": {
                "old_index": old_index,
                "new_index": document_analyzer.index_name,
                "index_changed": old_index != document_analyzer.index_name
            },
            "indexing_status": {
                "resume_indexed": resume_indexed,
                "job_indexed": job_indexed
            },
            "analysis_result": analysis_result
        }
        
        logger.info("âœ… ì—…ë¡œë“œ+ë¶„ì„ ì™„ë£Œ!")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ì—…ë¡œë“œ+ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì—…ë¡œë“œ+ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        )

@router.post("/upload-and-analyze-fast")
async def upload_and_analyze_fast_api(
    resume_file: UploadFile = File(...),
    job_file: UploadFile = File(...)
):
    """
    íŒŒì¼ ì—…ë¡œë“œ + ë¶„ì„ í•œ ë²ˆì— ì²˜ë¦¬ (ì‹œì—°ìš© ê³ ì† ëª¨ë“œ)
    ì¸ë±ì‹± ëŒ€ê¸° ì‹œê°„ì„ ìµœì†Œí™”í•˜ì—¬ ë¹ ë¥¸ ì‹œì—°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
    Args:
        resume_file: ì´ë ¥ì„œ íŒŒì¼
        job_file: ì±„ìš©ê³µê³  íŒŒì¼
        
    Returns:
        dict: ì—…ë¡œë“œ ê²°ê³¼ + ë¶„ì„ ê²°ê³¼
    """
    try:
        resume_filename = resume_file.filename or "unknown_resume.pdf"
        job_filename = job_file.filename or "unknown_job.pdf"
        logger.info(f"ğŸš€ ê³ ì† ì—…ë¡œë“œ+ë¶„ì„ ìš”ì²­: {resume_filename}, {job_filename}")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        resume_content = await resume_file.read()
        job_content = await job_file.read()
        
        # 1ë‹¨ê³„: ì—…ë¡œë“œ
        logger.info("ğŸ“¤ 1ë‹¨ê³„: íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
        resume_upload = upload_resume_file(resume_content, resume_filename)
        job_upload = upload_job_posting_file(job_content, job_filename)
        
        if resume_upload["status"] != "success" or job_upload["status"] != "success":
            return {
                "status": "error",
                "message": "íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨",
                "resume_upload": resume_upload,
                "job_upload": job_upload
            }
        
        # 2ë‹¨ê³„: ì¸ë±ì„œ ì¦‰ì‹œ ì‹¤í–‰
        logger.info("âš¡ 2ë‹¨ê³„: ì¸ë±ì„œ ì‹¤í–‰ ì¤‘...")
        indexer_result = document_analyzer.run_indexer()
        logger.info(f"   ì¸ë±ì„œ ì‹¤í–‰ ê²°ê³¼: {indexer_result.get('status', 'unknown')}")
        
        # 3ë‹¨ê³„: ì¸ë±ìŠ¤ ì¬ë°œê²¬
        logger.info("ğŸ” 3ë‹¨ê³„: ìµœì‹  ì¸ë±ìŠ¤ ì¬ë°œê²¬ ì¤‘...")
        old_index = document_analyzer.index_name
        document_analyzer.index_name = document_analyzer._get_active_index_name()
        
        # ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ ì—…ë°ì´íŠ¸
        from azure.search.documents import SearchClient
        document_analyzer.search_client = SearchClient(
            endpoint=document_analyzer.search_endpoint,
            index_name=document_analyzer.index_name,
            credential=document_analyzer.search_credential
        )
        
        if old_index != document_analyzer.index_name:
            logger.info(f"   ì¸ë±ìŠ¤ ë³€ê²½: {old_index} â†’ {document_analyzer.index_name}")
        else:
            logger.info(f"   ê¸°ì¡´ ì¸ë±ìŠ¤ ìœ ì§€: {document_analyzer.index_name}")
        
        # 4ë‹¨ê³„: ì§§ì€ ì¸ë±ì‹± ëŒ€ê¸° (ì‹œì—°ìš©: ìµœëŒ€ 10ì´ˆ)
        logger.info("â³ 4ë‹¨ê³„: ë¹ ë¥¸ ì¸ë±ì‹± í™•ì¸ ì¤‘...")
        from ..services.document_analyzer import wait_for_file_indexing
        import time
        
        resume_indexed = wait_for_file_indexing(f"resume_{resume_filename}", 10)
        job_indexed = wait_for_file_indexing(f"job_{job_filename}", 10)
        
        logger.info(f"   ë¹ ë¥¸ ì¸ë±ì‹± ìƒíƒœ - ì´ë ¥ì„œ: {resume_indexed}, ì±„ìš©ê³µê³ : {job_indexed}")
        
        # 5ë‹¨ê³„: ì¦‰ì‹œ ë¶„ì„ ì‹¤í–‰
        logger.info("ğŸ“Š 5ë‹¨ê³„: ì¦‰ì‹œ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
        analysis_result = analyze_candidate_match(resume_filename, job_filename)
        
        result = {
            "status": "success",
            "mode": "fast",
            "upload_results": {
                "resume_upload": resume_upload,
                "job_upload": job_upload
            },
            "indexer_result": indexer_result,
            "index_info": {
                "old_index": old_index,
                "new_index": document_analyzer.index_name,
                "index_changed": old_index != document_analyzer.index_name
            },
            "indexing_status": {
                "resume_indexed": resume_indexed,
                "job_indexed": job_indexed,
                "wait_time": "10_seconds_max"
            },
            "analysis_result": analysis_result
        }
        
        logger.info("âœ… ê³ ì† ì—…ë¡œë“œ+ë¶„ì„ ì™„ë£Œ!")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ê³ ì† ì—…ë¡œë“œ+ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê³ ì† ì—…ë¡œë“œ+ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        )

@router.get("/files-list")
async def get_files_list_api():
    """
    Azure Blob Storageì— ìˆëŠ” íŒŒì¼ ëª©ë¡ ì¡°íšŒ
    
    Returns:
        dict: ì´ë ¥ì„œ ë° ì±„ìš©ê³µê³  íŒŒì¼ ëª©ë¡
    """
    try:
        logger.info("íŒŒì¼ ëª©ë¡ ì¡°íšŒ ìš”ì²­")
        
        result = get_storage_files_list()
        
        logger.info(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ: {result.get('total_files', 0)}ê°œ")
        return result
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        ) 

@router.get("/debug-index")
async def debug_index_api():
    """
    Azure AI Search ì¸ë±ìŠ¤ ë””ë²„ê¹… (ê°œë°œìš©)
    
    Returns:
        dict: ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ ëª¨ë“  ë¬¸ì„œ ì •ë³´
    """
    try:
        logger.info("ì¸ë±ìŠ¤ ë””ë²„ê¹… ìš”ì²­")
        
        result = document_analyzer.debug_search_index()
        
        logger.info("ì¸ë±ìŠ¤ ë””ë²„ê¹… ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"ì¸ë±ìŠ¤ ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¸ë±ìŠ¤ ë””ë²„ê¹… ì‹¤íŒ¨: {str(e)}"
        ) 

@router.post("/run-indexer")
async def run_indexer_api():
    """
    Azure AI Search ì¸ë±ì„œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰ (Blob Storage â†’ AI Search ë™ê¸°í™”)
    
    Returns:
        dict: ì¸ë±ì„œ ì‹¤í–‰ ê²°ê³¼
    """
    try:
        logger.info("ì¸ë±ì„œ ìˆ˜ë™ ì‹¤í–‰ ìš”ì²­")
        
        result = document_analyzer.run_indexer()
        
        logger.info(f"ì¸ë±ì„œ ì‹¤í–‰ ì™„ë£Œ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"ì¸ë±ì„œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¸ë±ì„œ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        )

@router.get("/indexer-status")
async def get_indexer_status_api():
    """
    Azure AI Search ì¸ë±ì„œ ìƒíƒœ í™•ì¸
    
    Returns:
        dict: ëª¨ë“  ì¸ë±ì„œì˜ ìƒíƒœ ì •ë³´
    """
    try:
        logger.info("ì¸ë±ì„œ ìƒíƒœ í™•ì¸ ìš”ì²­")
        
        result = document_analyzer.check_indexer_status()
        
        logger.info("ì¸ë±ì„œ ìƒíƒœ í™•ì¸ ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"ì¸ë±ì„œ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¸ë±ì„œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}"
        ) 

@router.post("/integrated-analysis")
async def integrated_analysis_api(request: dict):
    """
    2ë‹¨ê³„ ì‹œì—°ìš©: ë¬¸ì„œ ë¶„ì„ + ë©´ì ‘ STT í†µí•© ë¶„ì„
    
    Args:
        request: {
            "document_analysis": "1ë‹¨ê³„ ë¬¸ì„œ ë¶„ì„ ê²°ê³¼",
            "interview_stt": "ë©´ì ‘ STT ê²°ê³¼",
            "resume_filename": "ì´ë ¥ì„œ íŒŒì¼ëª…",
            "job_filename": "ì±„ìš©ê³µê³  íŒŒì¼ëª…"
        }
        
    Returns:
        dict: ìµœì¢… ì¢…í•© í‰ê°€ ê²°ê³¼
    """
    try:
        document_analysis = request.get("document_analysis", "")
        interview_stt = request.get("interview_stt", "")
        resume_filename = request.get("resume_filename", "")
        job_filename = request.get("job_filename", "")
        
        logger.info("ğŸ”„ 2ë‹¨ê³„: ë¬¸ì„œ+ë©´ì ‘ í†µí•© ë¶„ì„ ì‹œì‘")
        
        if not document_analysis or not interview_stt:
            return {
                "status": "error",
                "message": "ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ì™€ ë©´ì ‘ STT ê²°ê³¼ê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤."
            }
        
        # í†µí•© ë¶„ì„ í”„ë¡¬í”„íŠ¸
        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ì±„ìš© ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ 1ë‹¨ê³„ ì„œë¥˜ ì‹¬ì‚¬ ê²°ê³¼ì™€ 2ë‹¨ê³„ ë©´ì ‘ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”.

## ğŸ“‹ 1ë‹¨ê³„: ì„œë¥˜ ì‹¬ì‚¬ ê²°ê³¼
{document_analysis}

## ğŸ¤ 2ë‹¨ê³„: ë©´ì ‘ ë‚´ìš© (STT ê²°ê³¼)
{interview_stt}

---

ì•„ë˜ í˜•ì‹ìœ¼ë¡œ **ìµœì¢… ì¢…í•© í‰ê°€**ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ¯ ìµœì¢… ì¢…í•© í‰ê°€

### ğŸ“Š ë‹¨ê³„ë³„ í‰ê°€ ìš”ì•½
- **ì„œë¥˜ ì‹¬ì‚¬**: [1ë‹¨ê³„ ê²°ê³¼ ìš”ì•½] 
- **ë©´ì ‘ í‰ê°€**: [ë©´ì ‘ ë‚´ìš© ê¸°ë°˜ í‰ê°€]
- **ì¢…í•© ì ìˆ˜**: XX/100ì 

### âœ… ìµœì¢… ê°•ì 
1. [ì„œë¥˜+ë©´ì ‘ì—ì„œ í™•ì¸ëœ í•µì‹¬ ê°•ì ]
2. [ì¼ê´€ì„± ìˆê²Œ ë‚˜íƒ€ë‚œ ì—­ëŸ‰]
3. [íŠ¹ë³„íˆ ì¸ìƒì ì¸ ë¶€ë¶„]

### âš ï¸ ìµœì¢… ìš°ë ¤ì‚¬í•­
1. [ì„œë¥˜ì™€ ë©´ì ‘ì—ì„œ ë°œê²¬ëœ gap]
2. [ë³´ì™„ í•„ìš”í•œ ì˜ì—­]
3. [ì¶”ê°€ ê²€ì¦ í•„ìš”í•œ ë¶€ë¶„]

### ğŸ’¼ ì±„ìš© ê¶Œê³ ì‚¬í•­
- **ìµœì¢… ê¶Œê³ **: [ì±„ìš© ê°•ë ¥ ì¶”ì²œ/ì¡°ê±´ë¶€ ì¶”ì²œ/ë³´ë¥˜/ë¶ˆí•©ê²©]
- **ë°°ì¹˜ ì¶”ì²œ ë¶€ì„œ**: [êµ¬ì²´ì  ë¶€ì„œëª… + ì´ìœ ]
- **ì˜¨ë³´ë”© ì‹œ ì£¼ì˜ì‚¬í•­**: [ì‹ ì…ì‚¬ì› ì ì‘ì„ ìœ„í•œ ì¡°ì–¸]

### ğŸ¯ ë©´ì ‘ê´€ì„ ìœ„í•œ ì¶”ê°€ í™•ì¸ ì§ˆë¬¸
1. [ê¸°ìˆ ì  ê¹Šì´ í™•ì¸ ì§ˆë¬¸]
2. [ë™ê¸°/ì—´ì • í™•ì¸ ì§ˆë¬¸]
3. [íŒ€ ì í•©ì„± í™•ì¸ ì§ˆë¬¸]

### ğŸ“ˆ ì„±ì¥ ê°€ëŠ¥ì„± ë° ì¥ê¸° ì „ë§
[í•´ë‹¹ ì§€ì›ìì˜ 3-5ë…„ í›„ ì„±ì¥ ê°€ëŠ¥ì„±ê³¼ íšŒì‚¬ ê¸°ì—¬ë„ ì˜ˆì¸¡]
"""
        
        # LLMì„ í†µí•œ í†µí•© ë¶„ì„
        result = document_analyzer.llm.invoke(prompt)
        
        return {
            "status": "success",
            "analysis_type": "integrated",
            "input_summary": {
                "document_analysis_length": len(document_analysis),
                "interview_stt_length": len(interview_stt),
                "resume_file": resume_filename,
                "job_file": job_filename
            },
            "integrated_analysis": result.content
        }
        
    except Exception as e:
        logger.error(f"âŒ í†µí•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            "status": "error",
            "message": f"í†µí•© ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        }

@router.post("/save-analysis-result")
async def save_analysis_result_api(request: dict):
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ Azure Blob Storageì— JSON íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        request: {
            "metadata": {
                "saved_at": "2024-01-15T10:30:00Z",
                "resume_file": "ì´ë ¥ì„œ íŒŒì¼ëª…",
                "job_file": "ì±„ìš©ê³µê³  íŒŒì¼ëª…", 
                "analysis_type": "document | integrated"
            },
            "results": {
                "document_analysis": "ë¬¸ì„œ ë¶„ì„ ê²°ê³¼",
                "interview_stt": "ë©´ì ‘ STT ê²°ê³¼",
                "integrated_analysis": "í†µí•© ë¶„ì„ ê²°ê³¼"
            }
        }
        
    Returns:
        dict: ì €ì¥ ê²°ê³¼
    """
    try:
        logger.info("ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ìš”ì²­")
        
        # ìš”ì²­ ë°ì´í„° ê²€ì¦
        if not request.get("metadata") or not request.get("results"):
            return {
                "status": "error",
                "message": "metadataì™€ resultsê°€ í•„ìš”í•©ë‹ˆë‹¤."
            }
        
        # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        analysis_type = request["metadata"].get("analysis_type", "unknown")
        filename = f"analysis_result_{analysis_type}_{timestamp}.json"
        
        # JSON ë°ì´í„° ì¤€ë¹„
        save_data = {
            "metadata": request["metadata"],
            "results": request["results"],
            "saved_info": {
                "filename": filename,
                "saved_at": datetime.datetime.now().isoformat(),
                "file_size": len(str(request))
            }
        }
        
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        import json
        json_content = json.dumps(save_data, ensure_ascii=False, indent=2)
        json_bytes = json_content.encode('utf-8')
        
        # Azure Blob Storageì— ì €ì¥
        result = document_analyzer.upload_file_to_storage(json_bytes, filename)
        
        if result["status"] == "success":
            logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
            return {
                "status": "success",
                "message": "ë¶„ì„ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "filename": filename,
                "saved_at": save_data["saved_info"]["saved_at"],
                "file_size": save_data["saved_info"]["file_size"]
            }
        else:
            logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {result['message']}")
            return {
                "status": "error",
                "message": f"ì €ì¥ ì‹¤íŒ¨: {result['message']}"
            }
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            "status": "error",
            "message": f"ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        }

@router.get("/get-saved-results")
async def get_saved_results_api():
    """
    ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ
    
    Returns:
        dict: ì €ì¥ëœ ê²°ê³¼ íŒŒì¼ ëª©ë¡
    """
    try:
        logger.info("ğŸ“‹ ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ")
        
        # ì „ì²´ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        files_result = document_analyzer.get_blob_files_list()
        
        if files_result["status"] != "success":
            return {
                "status": "error",
                "message": "íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨"
            }
        
        # ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤ë§Œ í•„í„°ë§ (analysis_result_ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ë“¤)
        all_files = files_result.get("files", [])
        result_files = []
        
        for file_info in all_files:
            filename = file_info.get("name", "")
            if filename.startswith("analysis_result_") and filename.endswith(".json"):
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œë„
                try:
                    # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    parts = filename.replace("analysis_result_", "").replace(".json", "").split("_")
                    if len(parts) >= 2:
                        analysis_type = parts[0]
                        timestamp = "_".join(parts[1:])
                        
                        result_files.append({
                            "filename": filename,
                            "metadata": {
                                "analysis_type": analysis_type,
                                "timestamp": timestamp,
                                "saved_at": file_info.get("last_modified", ""),
                                "file_size": file_info.get("size", 0)
                            }
                        })
                except Exception as e:
                    logger.warning(f"íŒŒì¼ëª… íŒŒì‹± ì˜¤ë¥˜: {filename} - {str(e)}")
                    # ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ê°€
                    result_files.append({
                        "filename": filename,
                        "metadata": {
                            "analysis_type": "unknown",
                            "timestamp": "",
                            "saved_at": file_info.get("last_modified", ""),
                            "file_size": file_info.get("size", 0)
                        }
                    })
        
        # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
        result_files.sort(key=lambda x: x["metadata"]["saved_at"], reverse=True)
        
        logger.info(f"âœ… ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ {len(result_files)}ê°œ ë°œê²¬")
        return {
            "status": "success",
            "total_results": len(result_files),
            "results": result_files
        }
        
    except Exception as e:
        logger.error(f"âŒ ì €ì¥ëœ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            "status": "error",
            "message": f"ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        }

@router.get("/load-analysis-result/{filename}")
async def load_analysis_result_api(filename: str):
    """
    ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
    
    Args:
        filename: ë¶ˆëŸ¬ì˜¬ ê²°ê³¼ íŒŒì¼ëª…
        
    Returns:
        dict: ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ë°ì´í„°
    """
    try:
        logger.info(f"ğŸ“‚ ë¶„ì„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°: {filename}")
        
        # íŒŒì¼ëª… ê²€ì¦
        if not filename.startswith("analysis_result_") or not filename.endswith(".json"):
            return {
                "status": "error",
                "message": "ì˜¬ë°”ë¥¸ ë¶„ì„ ê²°ê³¼ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤."
            }
        
        # Azure Blob Storageì—ì„œ íŒŒì¼ ì½ê¸°
        if document_analyzer.blob_service_client is None:
            return {
                "status": "error",
                "message": "Azure Storageê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        blob_client = document_analyzer.blob_service_client.get_blob_client(
            container=document_analyzer.container_name,
            blob=filename
        )
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        try:
            blob_data = blob_client.download_blob()
            json_content = blob_data.readall().decode('utf-8')
            
            # JSON íŒŒì‹±
            import json
            result_data = json.loads(json_content)
            
            logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: {filename}")
            return {
                "status": "success",
                "filename": filename,
                "data": result_data,
                "loaded_at": datetime.datetime.now().isoformat()
            }
            
        except Exception as e:
            if "BlobNotFound" in str(e):
                return {
                    "status": "error",
                    "message": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            else:
                raise e
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            "status": "error",
            "message": f"ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"
        }

@router.delete("/delete-analysis-result/{filename}")
async def delete_analysis_result_api(filename: str):
    """
    ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ì‚­ì œ
    
    Args:
        filename: ì‚­ì œí•  ê²°ê³¼ íŒŒì¼ëª…
        
    Returns:
        dict: ì‚­ì œ ê²°ê³¼
    """
    try:
        logger.info(f"ğŸ—‘ï¸ ë¶„ì„ ê²°ê³¼ ì‚­ì œ: {filename}")
        
        # íŒŒì¼ëª… ê²€ì¦
        if not filename.startswith("analysis_result_") or not filename.endswith(".json"):
            return {
                "status": "error",
                "message": "ì˜¬ë°”ë¥¸ ë¶„ì„ ê²°ê³¼ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤."
            }
        
        # Azure Blob Storageì—ì„œ íŒŒì¼ ì‚­ì œ
        if document_analyzer.blob_service_client is None:
            return {
                "status": "error",
                "message": "Azure Storageê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        blob_client = document_analyzer.blob_service_client.get_blob_client(
            container=document_analyzer.container_name,
            blob=filename
        )
        
        # íŒŒì¼ ì‚­ì œ
        try:
            blob_client.delete_blob()
            
            logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì‚­ì œ ì™„ë£Œ: {filename}")
            return {
                "status": "success",
                "message": "ë¶„ì„ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "filename": filename,
                "deleted_at": datetime.datetime.now().isoformat()
            }
            
        except Exception as e:
            if "BlobNotFound" in str(e):
                return {
                    "status": "error",
                    "message": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            else:
                raise e
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            "status": "error",
            "message": f"ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
        } 